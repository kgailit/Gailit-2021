{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "understanding-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import string \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "welsh-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sisendiks faili nimi ja praegune kaust ning sihtkohtade kaustad\n",
    "def morphanalysis_and_save(filename, source, guess_destination, no_guess_destination, meta_destination):\n",
    "    \n",
    "    global keskmiste_arvutamiseks\n",
    "    \n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "        faili_keskmised = defaultdict(int)\n",
    "        \n",
    "        # Loeb failist vaid sisu, ignoreerib alguses olevat metainfot\n",
    "        pure = \"\".join(f.readlines()[1:])\n",
    "        \n",
    "        # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "        emojid = re.findall(\":\\S+?:\", pure)\n",
    "        # Eemaldab leitud emojide hulgast väärvasted\n",
    "        wrong = [\":http:\", \":https:\"]\n",
    "        for value in wrong:\n",
    "            while value in emojid:\n",
    "                emojid.remove(value)\n",
    "        # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "        for emoji in emojid:\n",
    "            pure = pure.replace(emoji, \"\")\n",
    "\n",
    "        # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "        emotikonid_leitud = []\n",
    "        for emotikon in emotikonid:\n",
    "            emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "            # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "            pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "        # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "        # Lisaks eemaldad leitud emotikonid\n",
    "        sobivad = []\n",
    "        for emotikon in emotikonid_probleemsed:\n",
    "            # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "            if emotikon[-1] == \":\":\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "            # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "            else:\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "        \n",
    "        # Jätab meelde emotikonide arvud ja loendid vigade kontrollimiseks\n",
    "        faili_keskmised['emotikonide_arv']=len(emojid) + len(emotikonid_leitud) + len(sobivad)\n",
    "        faili_keskmised['emojid_loend']=emojid\n",
    "        faili_keskmised['emotikonid_loend']=emotikonid_leitud\n",
    "        faili_keskmised['emotikonid_erilised_loend']=sobivad\n",
    "        \n",
    "        # Teeb morfoloogilise analüüsi nii tundmatude analüüsi oletamisega ja oletamiseta\n",
    "        oletamisega = Text(pure)\n",
    "        oletamiseta = Text(pure, disambiguate=False, guess=False, propername=False)\n",
    "\n",
    "        oletamisega.tag_analysis()\n",
    "        oletamiseta.tag_analysis()\n",
    "        \n",
    "        # Loob salvestamiseks failinimed\n",
    "        ga_name = \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamisega.json\"\n",
    "        ta_name = \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamiseta.json\"\n",
    "        meta_name = \".\".join(filename.split(\".\")[:-1]) + \"_meta.json\"\n",
    "        \n",
    "        with open(os.path.join(guess_destination, ga_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(oletamisega, fp, sort_keys=True, indent=4)\n",
    "        with open(os.path.join(no_guess_destination, ta_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(oletamiseta, fp, sort_keys=True, indent=4)\n",
    "\n",
    "        kõikide_lemmade_arv = 0\n",
    "        ainult_käänduvate_lemmade_arv = 0\n",
    "            \n",
    "        for lemma, postag in zip(oletamisega.lemmas, oletamisega.postags):\n",
    "            kõikide_lemmade_arv += 1\n",
    "            keskmiste_arvutamiseks['kõikide_lemmade_arv'] += 1\n",
    "            if postag in [\"A\", \"C\", \"G\", \"H\", \"K\", \"N\", \"O\", \"S\", \"U\", \"Y\"]:\n",
    "                ainult_käänduvate_lemmade_arv += 1\n",
    "                keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv'] += 1\n",
    "\n",
    "        lemmas_subwords = []\n",
    "\n",
    "        for tokens in oletamisega.root_tokens:\n",
    "            # Kui tegu on sõnega ja mitte loendiga\n",
    "            if isinstance(tokens[0], str):\n",
    "                for token in tokens:\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "            # Kui tegu on loendiga, võtab esimese tõlgenduse\n",
    "            else:\n",
    "                for token in tokens[0]:\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "                        \n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_pikkused'] += sum(map(len, lemmas_subwords))\n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_arv'] += len(lemmas_subwords)\n",
    "        \n",
    "        lemmad = [lemma.split(\"|\")[0] for lemma in oletamisega.lemmas if lemma not in string.punctuation]\n",
    "        \n",
    "        faili_keskmised['TTR'] = len(Counter(lemmad))/len(lemmad)\n",
    "        faili_keskmised['keskmine_lemma_pikkus'] = sum(map(len, lemmas_subwords))/len(lemmas_subwords)\n",
    "        faili_keskmised['käänduvate_lemmade_osaarv'] = ainult_käänduvate_lemmade_arv/kõikide_lemmade_arv\n",
    "        \n",
    "        keskmiste_arvutamiseks['TTR_keskmine'] += faili_keskmised['TTR']\n",
    "        \n",
    "        with open(os.path.join(meta_destination, meta_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(faili_keskmised, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "opponent-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\"]:\n",
    "    with open(\"Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            emotikonid.append(line.strip().lower())\n",
    "            \n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "emotikonid_probleemsed = []\n",
    "\n",
    "with open(\"Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "        \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "substantial-nutrition",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-2677156fbc3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mkeskmised\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mkeskmised\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TTR'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeskmiste_arvutamiseks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TTR_kokku'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mkeskmised\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keskmine_lemma_pikkus'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeskmiste_arvutamiseks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmade_osasõnade_pikkused'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeskmiste_arvutamiseks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmade_osasõnade_arv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mkeskmised\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'käänduvate_lemmade_osaarv'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeskmiste_arvutamiseks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ainult_käänduvate_lemmade_arv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeskmiste_arvutamiseks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'kõikide_lemmade_arv'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "source = \"etnc19_web_2019_100000/\"\n",
    "target1 = \"etnc19_web_2019_morf_oletamisega/\"\n",
    "target2 = \"etnc19_web_2019_morf_oletamiseta/\"\n",
    "target3 = \"etnc19_web_2019_meta/\"\n",
    "\n",
    "os.makedirs(os.path.dirname(target1), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(target2), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(target3), exist_ok=True)\n",
    "\n",
    "keskmiste_arvutamiseks = defaultdict(int)\n",
    "\n",
    "for file in [f for f in os.listdir(source)]:\n",
    "    morphanalysis_and_save(file, source, target1, target2, target3)\n",
    "    \n",
    "keskmised = defaultdict(float)\n",
    "\n",
    "keskmised['TTR'] = keskmiste_arvutamiseks['TTR_keskmine']/100000\n",
    "keskmised['keskmine_lemma_pikkus'] = keskmiste_arvutamiseks['lemmade_osasõnade_pikkused']/keskmiste_arvutamiseks['lemmade_osasõnade_arv']\n",
    "keskmised['käänduvate_lemmade_osaarv'] = keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv']/keskmiste_arvutamiseks['kõikide_lemmade_arv']\n",
    "\n",
    "with open(\"keskmised.json\", \"w\", encoding = \"utf-8\") as fw: \n",
    "    json.dump(keskmised, fw, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-density",
   "metadata": {},
   "source": [
    "Vana info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "suburban-period",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keskmine sõnapikkus (liitsõnad osadena): 4.27369826435247\n",
      "Type-Token ratio                       : 0.4489510489510489\n",
      "Käändsõna osakaal                      : 0.2987951807228916\n"
     ]
    }
   ],
   "source": [
    "#Andmed\n",
    "print('Keskmine sõnapikkus (liitsõnad osadena): {}'.format(avg_lemma_len))\n",
    "print('Type-Token ratio                       : {}'.format(TTR))\n",
    "print('Käändsõna osakaal                      : {}'.format(kaanduv_suhe_koik))\n",
    "#print('Tajuverbide osakaal verbidest          : {}'.format(tajuverbid_suhe_all_verbs)) # Vaja alles üksikteksti analüüsiks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-syndrome",
   "metadata": {},
   "source": [
    "Tajuverbide keskmise arvutamine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "thick-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Loendid\\\\tajuverbid\\wordnet_tajuverbid.txt\", \"r\", encoding = \"utf8\") as fr:\n",
    "    lines = fr.readlines()\n",
    "    tajuverbid = [verb.strip() for verb in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "engaged-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = 0\n",
    "only_tajuverbs = 0\n",
    "\n",
    "for lemma, postag in zip(oletamisega.lemmas, oletamisega.postags):\n",
    "    if postag == \"V\":\n",
    "        all_verbs += 1\n",
    "        if lemma in tajuverbid:\n",
    "            only_tajuverbs += 1\n",
    "\n",
    "tajuverbid_suhe_all_verbs = only_tajuverbs / all_verbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
